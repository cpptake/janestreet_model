{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code]\n\n\n# %% [markdown]\n# ### I implemented feature neutralization suggested by [this kernel](https://www.kaggle.com/code1110/janestreet-avoid-overfit-feature-neutralization) over the awesome [keras kernel](https://www.kaggle.com/tarlannazarov/own-jane-street-with-keras-nn) then added a bunch of plots for cross validation. \n\n# %% [markdown]\n# \n# # CV結果\n# \n# # ver18\n# ## best pram ニュートライ図アリ\n# \n# FOLD 0\n# ROC AUC:\t 0.5460606911248014\t SCORE: 2289.622246284669\n# \n# FOLD 1\n# ROC AUC:\t 0.5594050610055324\t SCORE: 3299.472512658139\n# \n# FOLD 2\n# ROC AUC:\t 0.5680242535452669\t SCORE: 4335.480807082125\n# \n# FOLD 3\n# ROC AUC:\t 0.5747448485224484\t SCORE: 3059.614618212791\n# \n# FOLD 4\n# ROC AUC:\t 0.5835644934134365\t SCORE: 5894.3296893748075\n# \n# \n# AUC:0.5661277105956023\n# \n# 30iter/secくらい\n# \n# 多分無理\n# \n# \n# \n# \n# # ver 17\n# ## best paramのニュートライズなし\n# \n# epochs = 200\n# batch_size = 4096\n# hidden_units = [272, 304, 512]\n# dropout_rates = [0.05, 0.45, 0.1, 0.3]\n# ただし、earlystopping=50\n# \n# \n# FOLD 0 \n# ROC AUC:\t 0.5461643588538437\t SCORE: 2408.835743650814\n# \n# FOLD 1\n# ROC AUC:\t 0.5595333663014118\t SCORE: 3593.59249431354\n# \n# FOLD 2\n# ROC AUC:\t 0.5681268656824688\t SCORE: 4347.63811100712\n# \n# FOLD 3\n# ROC AUC:\t 0.575235301757587\t SCORE: 3151.4888590276605\n# \n# FOLD 4\n# ROC AUC:\t 0.5839188378905474\t SCORE: 6027.130374779819\n# \n# \n# AUC:0.56627\n# \n# \n# 39iter/sec\n# \n# 微妙\n# \n# \n# \n# \n# \n# # ver16\n# ## ver1 + early stopping +　ニュートライズなし　のUtility計算\n# \n# \n# FOLD 0\n# ROC AUC:\t 0.547594528087585\t SCORE: 2426.20931427235\n# \n# FOLD 1\n# ROC AUC:\t 0.556370440863294\t SCORE: 2751.246210427712\n# \n# FOLD 2\n# ROC AUC:\t 0.5595430471590683\t SCORE: 3261.752867456613\n# \n# FOLD 3\n# ROC AUC:\t 0.562053729557124\t SCORE: 1498.3005090815\n# \n# FOLD 4\n# ROC AUC:\t 0.5682277497197191\t SCORE: 4411.4055512860\n# \n# AUC: 0.55865\n# \n# \n# \n# \n# \n# \n# # ver15\n# ## ver9 + early stopping +　ニュートライズなし　のUtility計算\n# \n# FOLD 0\n# ROC AUC:\t 0.5472964659403246\t SCORE: 2329.6900327064955\n# \n# FOLD 1\n# ROC AUC:\t 0.5587354353333415\t SCORE: 3117.133004587549\n# \n# FOLD 2\n# ROC AUC:\t 0.5670219968860197\t SCORE: 4083.796559755803\n# \n# FOLD 3\n# ROC AUC:\t 0.5715675465750136\t SCORE: 2870.7543732032323\n# \n# FOLD 4\n# AUC:\t 0.580726738313501\t SCORE: 5575.305543031929\n# \n# AUC 0.565135\n# \n# \n# 40iter/secくらい\n# \n# ぎりぎり\n# \n# \n# \n# \n# # ver14\n# \n# ## ver7 + early stopping +　ニュートライズなし　のUtility計算\n# \n# FOLD 0\n# ROC AUC:\t 0.547594513422818\t SCORE: 2426.20338839941\n# \n# FOLD 1\n# ROC AUC:\t 0.5563704714459596\t SCORE: 2750.8520995790436\n# \n# FOLD 2\n# ROC AUC:\t 0.559542994245273\t SCORE: 3261.251888426018\n# \n# FOLD 3\n# ROC AUC:\t 0.5620537173899884\t SCORE: 1498.3005090815163\n# \n# FOLD 4\n# ROC AUC:\t 0.5682277940601879\t SCORE: 4411.490339240615\n# \n# \n# AUC:0.5586554198423483\n# \n# \n# \n# \n# # ver13\n# \n# ## ver1  + early stopping のUtility\n# \n# Fold1\n# ROC AUC:\t 0.5476\t SCORE: 2517.269\n# \n# Fold2\n# ROC AUC:\t 0.5565  Score: 2773.22\n# \n# FOLD3\n# ROC AUC:\t 0.5597667768809174\t SCORE: 3180.1481908872\n# \n# FOLD 4\n# ROC AUC:\t 0.562176041388657\t SCORE: 1276.3162299919538\n# \n# FOLD 5\n# ROC AUC:\t 0.5686575298953652\t SCORE: 4469.523819442379\n# \n# \n# 平均35iter/secくらい\n# 重そう\n# \n# \n# # ver 12\n# ## ver9 + early stopping のUtility\n# \n# Fold1\n# AUC:\t 0.5472738332113048\t SCORE: 2318.954025177425\n# \n# FOLD 2\n# ROC AUC:\t 0.5585913252222039\t SCORE: 3173.2861346620393\n# \n# FOLD 3\n# ROC AUC:\t 0.5671254574565872\t SCORE: 4149.479338054305\n# \n# FOLD 4\n#     AUC:     0.5719              SCORE: 2833.88\n#     \n# FOLD 5\n# ROC AUC:\t 0.580561945171171\t SCORE: 5476.085222087269\n# \n# 40iter/secくらい\n# \n# ぎりぎり\n# \n# \n# \n# ## ver7のUtility計算\n# \n# Utility\n# \n# FOLD1  Utility : 3190.136406655242     AUC:0.54930\n# \n# FOLD2  Utility : 3631.2769014130963    AUC:0.56526\n# \n# FOLD3  Utility : 3482.101481628325     AUC:0.561823 \n# \n# FOLD4  Utility : 880.3798910450603　　　AUC:0.556138 \n# \n# FOLD5  Utility： 3261.1141994927393　　AUC:0.561027\n# \n# \n# ## ver10\n# \n# ver6と同じ。\n# Timeoutのため再チャレンジ\n# \n# AUC：0.5567\n# \n# \n# ## ver9\n# \n# ニュートライズあり\n# \n# ##ver9\n# batch_size = 4096\n# hidden_units = [192, 384, 192]\n# dropout_rates = [0.10143786981358652, 0.2703017847244654, 0.23148340929571917, 0.2357768967777311]\n# label_smoothing = 1e-2\n# learning_rate = 1e-3\n# epochs = 200\n# \n# \n# \n# \n# ## ver8\n# ver2と同じ再挑戦\n# \n# batch_size = 4096\n# hidden_units = [192, 384, 192]\n# dropout_rates = [0.10143786981358652, 0.2703017847244654, 0.23148340929571917, 0.2357768967777311]\n# label_smoothing = 1e-2\n# learning_rate = 1e-3\n# epochs = 200\n# \n# \n# \n# ## ver7\n# \n# ニュートライズなし\n# \n# epochs = 200\n# batch_size = 4096\n# hidden_units = [160, 160, 160]\n# dropout_rates = [0.2, 0.2, 0.2, 0.2]\n# label_smoothing = 1e-2\n# learning_rate = 1e-3\n# \n# AUC:0.5569\n# \n# score:\n# \n# \n# \n# ## ver6\n# \n# \n# ニュートライズあり\n# \n# epochs = 200\n# batch_size = 4096\n# hidden_units = [160, 160, 160]\n# dropout_rates = [0.2, 0.2, 0.2, 0.2]\n# label_smoothing = 1e-2\n# learning_rate = 1e-3\n# \n# AUC 0.55671\n# \n# score:たいむあうとおおおおおおおおおおおおおおお\n# \n# \n# ## ver3 \n# パラメータ変更\n# \n# AUC :　0.6019\n# \n# batch_size = 4096\n# hidden_units = [384, 896, 896, 394]\n# dropout_rates = [0.10143786981358652, 0.19720339053599725, 0.2703017847244654, 0.23148340929571917, 0.2357768967777311]\n# label_smoothing = 1e-2\n# learning_rate = 1e-3\n# epochs = 200\n# \n# \n# \n# ## ver2\n# パラメータ変更\n# \n# batch_size = 4096\n# hidden_units = [384, 896, 384]\n# dropout_rates = [0.10143786981358652, 0.2703017847244654, 0.23148340929571917, 0.2357768967777311]\n# label_smoothing = 1e-2\n# learning_rate = 1e-3\n# epochs = 200\n# \n# OOF全体のAUC　0.584003\n# \n# \n# ## ver1\n# \n# \n# fold0 : AUC　0.548\n# \n# flod1 : AUC 0.5548\n# \n# fold2 : AUC 0.5555\n# \n# fold3 : AUC 0.5568\n# \n# fold4 : AUC 0.5644\n# \n# \n# OOF全体のAUC　0.5558\n\n# %% [code]\nfrom tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport pickle\nfrom tqdm import tqdm\nfrom random import choices\n\n\nSEED = 1111\ninference = False\ncv = False\n\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\n\n#train_pickle_file = '/kaggle/input/pickling/train.csv.pandas.pickle'\ntrain_pickle_file = '../input/janestreettraincsvpickling/train.csv.pandas.pickle'\ntrain = pickle.load(open(train_pickle_file, 'rb'))\n#train = pd.read_csv('../input/jane-street-market-prediction/train.csv')\n\ntrain = train.query('date > 85').reset_index(drop = True) \ntrain = train[train['weight'] != 0]\n\ntrain.fillna(train.mean(),inplace=True)\n\ntrain['action'] = ((train['resp'].values) > 0).astype(int)\ntrain['bias'] = 1\n\n\nfeatures = [c for c in train.columns if \"feature\" in c]\n\n# %% [code]\ntrain.head()\n\n# %% [markdown]\n# ## Feature Neutralization\n\n# %% [code]\n\n\n# code to feature neutralize\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm.auto import tqdm\n\ndef build_neutralizer(train, features, proportion, return_neut=False):\n    #Builds neutralzied features, then trains a linear model to predict neutralized features from original\n    #features and return the coeffs of that model.\n    neutralizer = {}\n    neutralized_features = np.zeros((train.shape[0], len(features)))\n    target = train[['resp', 'bias']].values\n    for i, f in enumerate(features):\n        # obtain corrected feature\n        feature = train[f].values.reshape(-1, 1)\n        coeffs = np.linalg.lstsq(target, feature)[0]\n        neutralized_features[:, i] = (feature - (proportion * target.dot(coeffs))).squeeze()\n        \n    # train model to predict corrected features\n    neutralizer = np.linalg.lstsq(train[features+['bias']].values, neutralized_features)[0]\n    \n    if return_neut:\n        return neutralized_features, neutralizer\n    else:\n        return neutralizer\n\ndef neutralize_array(array, neutralizer):\n    neutralized_array = array.dot(neutralizer)\n    return neutralized_array\n\n\ndef test_neutralization():\n    dummy_train = train.loc[:100000, :]\n    proportion = 1.0\n    neutralized_features, neutralizer = build_neutralizer(dummy_train, features, proportion, True)\n    dummy_neut_train = neutralize_array(dummy_train[features+['bias']].values, neutralizer)\n    \n#     assert np.array_equal(neutralized_features, dummy_neut_train)\n    print(neutralized_features[0, :10], dummy_neut_train[0, :10])\n    \n\ntest_neutralization()\n\n\n# %% [markdown]\n# **We can see that it almost predicts it correctly and the offset isn't that huge.**\n\n# %% [code]\n\nproportion = 1.0\n\nneutralizer = build_neutralizer(train, features, proportion)\ntrain[features] = neutralize_array(train[features+['bias']].values, neutralizer)\n\n\n# %% [code]\nf_mean = np.mean(train[features[1:]].values,axis=0)\n\nresp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\n\nX = train.loc[:, train.columns.str.contains('feature')]\n#y_train = (train.loc[:, 'action'])\n\ny = np.stack([(train[c] > 0).astype('int') for c in resp_cols]).T\n\n# %% [markdown]\n# ## Model Training\n\n# %% [code]\ndef create_mlp(\n    num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate\n):\n\n    inp = tf.keras.layers.Input(shape=(num_columns,))\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n    for i in range(len(hidden_units)):\n        x = tf.keras.layers.Dense(hidden_units[i])(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n        x = tf.keras.layers.Dropout(dropout_rates[i + 1])(x)\n\n    x = tf.keras.layers.Dense(num_labels)(x)\n    out = tf.keras.layers.Activation(\"sigmoid\")(x)\n\n    model = tf.keras.models.Model(inputs=inp, outputs=out)\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing),\n        metrics=tf.keras.metrics.AUC(name=\"AUC\"),\n    )\n\n    return model\n\n\n# パラメータ\n\n##ver17\n##optuna\n#epochs = 200\n#batch_size = 4096\n#hidden_units = [272, 304, 512]\n#dropout_rates = [0.05, 0.45, 0.1, 0.3]\n#label_smoothing = 1e-2\n#learning_rate = 1e-3\n\n\n##ver1\n#batch_size = 5000\n#hidden_units = [150, 150, 150]\n#dropout_rates = [0.2, 0.2, 0.2, 0.2]\n#label_smoothing = 1e-2\n#learning_rate = 1e-3\n#epochs = 200\n\n##ver2,7\n#batch_size = 4096\n#hidden_units = [384, 896, 384]\n#dropout_rates = [0.10143786981358652, 0.2703017847244654, 0.23148340929571917, 0.2357768967777311]\n#label_smoothing = 1e-2\n#learning_rate = 1e-3\n#epochs = 200\n\n\n##ver3\n#batch_size = 4096\n#hidden_units = [384, 896, 896, 394]\n#dropout_rates = [0.10143786981358652, 0.19720339053599725, 0.2703017847244654, 0.23148340929571917, 0.2357768967777311]\n#label_smoothing = 1e-2\n#learning_rate = 1e-3\n#epochs = 200\n\n##ver4,ver7\n#epochs = 200\n#batch_size = 4096\n#hidden_units = [160, 160, 160]\n#dropout_rates = [0.2, 0.2, 0.2, 0.2]\n#label_smoothing = 1e-2\n#learning_rate = 1e-3\n\n##ver9\nbatch_size = 4096\nhidden_units = [192, 384, 192]\ndropout_rates = [0.10143786981358652, 0.2703017847244654, 0.23148340929571917, 0.2357768967777311]\nlabel_smoothing = 1e-2\nlearning_rate = 1e-3\nepochs = 200\n\n\n# %% [markdown]\n# ## Cross Validation using GroupKFold\n\n# %% [code]\ndef utility_score_bincount(date, weight, resp, action): \n    count_i = len(np.unique(date))\n    # print('weight: ', weight)\n    # print('resp: ', resp)\n    # print('action: ', action)\n    # print('weight * resp * action: ', weight * resp * action)\n    Pi = np.bincount(date, weight * resp * action)\n    t = np.sum(Pi) / np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 / count_i)\n    u = np.clip(t, 0, 6) * np.sum(Pi)\n    return u\n\n# %% [code]\ntrain\n\n# %% [code]\ncv = True\nth = 0.5\n\nclf = create_mlp(\n    len(features), 5, hidden_units, dropout_rates, label_smoothing, learning_rate\n    )\n\nmodels = []\n\n\nif cv:\n\n    from sklearn.model_selection import GroupKFold\n    from sklearn.metrics import mean_squared_error, roc_auc_score, roc_curve, precision_recall_curve\n    import gc\n\n    # oof validation probability array\n    oof_probas = np.zeros(y.shape)\n\n    # validation indices in case of time series split\n    val_idx_all = []\n\n    # cv strategy\n    N_SPLITS = 5\n    gkf = GroupKFold(n_splits=N_SPLITS)\n    \n    \n    ###kenkonishi model utility計算版\n    for fold, (train_idx, val_idx) in enumerate(gkf.split(train.action.values, groups=train.date.values)):\n\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx].values\n        y_train, y_val = y[train_idx], y[val_idx]\n        \n        print(f'FOLD {fold}回目',fold)\n        # training and evaluation score\n        c_filepath = f'./keras_nn_fold{fold}.hdf5'\n        cp_callback = tf.keras.callbacks.ModelCheckpoint(c_filepath, \n#                                                          monitor='val_acc',\n                                                    save_best_only=True,\n                                                    save_weights_only=True,\n                                                    )\n        es = EarlyStopping(patience=50)\n        \n        clf.fit(X_train,\n                y_train,\n                validation_data=(X_val, y_val),\n                epochs=epochs,\n                batch_size=4096,\n                callbacks=[cp_callback, es])\n        clf.save(f'./keras_nn_fold{fold}.hdf5',fold)\n        #clf.load_weights(c_filepath)\n        models.append(clf)\n\n        oof_probas[val_idx] += clf(X_val, training=False).numpy()\n\n        score = roc_auc_score(y_val, oof_probas[val_idx])  # classification score\n        \n        #####################################################################################################\n        valid_score = utility_score_bincount(\n            date=train.iloc[val_idx].date.values,\n            weight=train.iloc[val_idx].weight.values,\n            resp=train.iloc[val_idx].resp.values,\n            action=np.where(np.mean(oof_probas[val_idx], axis=1)>th, 1, 0))\n        print(f'FOLD {fold} ROC AUC:\\t {score}\\t SCORE: {valid_score}')\n\n        # deleting excess data to avoid running out of memory\n        del X_train, X_val, y_train, y_val\n        gc.collect()\n\n        # appending val_idx in case of group time series split\n        val_idx_all.append(val_idx)\n    \n    #\n    #lf.save('keras_nn_cv.hdf5')\n    \n    # concatenation of all val_idx for further acessing\n    val_idx = np.concatenate(val_idx_all)\n\n    \nelse:\n    \n    KFOLD = 4\n    modelpath = '../input/nn-with-features-neutralization-ver12/keras_nn_fold'\n    modelfile = '.hdf5'\n    \n    for fold in range(KFOLD):\n        \n        modelno = fold\n        bbb = modelpath + str(modelno) + modelfile\n        clf.load_weights(bbb)\n        models.append(clf)\n        \n        print('weight load Done!')    \n\n\n\n\n\n\n# %% [markdown]\n# ## ROC AUC\n\n# %% [code]\nif cv:\n    auc_oof = roc_auc_score(y[val_idx], oof_probas[val_idx])\n    print(auc_oof)\n\n# %% [markdown]\n# ## Helper functions\n\n# %% [code] {\"_kg_hide-input\":true}\nimport matplotlib.pyplot as plt\n\ndef determine_action(df, thresh):\n    \"\"\"Determines action based on defined threshold.\"\"\"\n    action = (df.weight * df.resp > thresh).astype(int)\n    return action\n\ndef date_weighted_resp(df):\n    \"\"\"Calculates the sum of weight, resp, action product.\"\"\"\n    cols = ['weight', 'resp', 'action']\n    weighted_resp = np.prod(df[cols], axis=1)\n    return weighted_resp.sum()\n\ndef calculate_t(dates_p):\n    \"\"\"Calculate t based on dates sum of weighted returns\"\"\"\n    e_1 =  dates_p.sum() / np.sqrt((dates_p**2).sum())\n    e_2 = np.sqrt(250/np.abs(len(dates_p)))\n    return e_1 * e_2\n\ndef calculate_u(df, thresh):\n    \"\"\"Calculates utility score, and return t and u.\"\"\"\n    df = df.copy()\n\n    # calculates sum of dates weighted returns\n    dates_p = df.groupby('date').apply(date_weighted_resp)\n        \n    # calculate t\n    t = calculate_t(dates_p)\n    return t, min(max(t, 0), 6) * dates_p.sum()\n\n\n\ndef plot_roc_curve(fpr, tpr, label=None):\n    plt.figure(figsize=(8, 6))\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'k--', label='Random')  # dashed diagonal\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.legend(loc='lower right')\n    plt.grid()\n    \n    \ndef plot_precision_recall_curve(precisions, recalls, thresholds):\n    plt.figure(figsize=(8, 6))\n    plt.plot(thresholds, precisions[:-1], 'b--', label='Precision')\n    plt.plot(thresholds, recalls[:-1], 'g-', label='Recall')\n    plt.xlabel('Thresholds')\n    plt.legend(loc='lower left')\n    plt.grid()\n\n\n    \ndef plot_thresh_u_t(df, oof):\n    threshs = np.linspace(0, 1, 1000)\n    ts = []\n    us = []\n    \n    for thresh in threshs:\n        df['action'] = np.where(oof >= thresh, 1, 0)\n        t, u = calculate_u(df, thresh)\n        ts.append(t)\n        us.append(u)\n        \n    # change nans into 0\n    ts = np.array(ts)\n    us = np.array(us)\n    ts = np.where(np.isnan(ts), 0.0, ts)\n    us = np.where(np.isnan(us), 0.0, us)\n    \n    tmax = np.argmax(ts)\n    umax = np.argmax(us)\n    \n    print(f'Max Utility Score: {us[umax]}')\n    \n    fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n    axes[0].plot(threshs, ts)\n    axes[0].set_title('Different t scores by threshold')\n    axes[0].set_xlabel('Threshold')\n    axes[0].axvline(threshs[tmax])\n\n    axes[1].plot(threshs, us)\n    axes[1].set_title('Different u scores by threshold')\n    axes[1].set_xlabel('Threshold')\n    axes[1].axvline(threshs[umax], color='r', linestyle='--', linewidth=1.2)\n    \n    print(f'Optimal Threshold: {threshs[umax]}')\n    \n    return threshs[umax]\n\n\n# %% [markdown]\n# ## ROC Curve\n\n# %% [code]\nif cv:\n    fpr, tpr, thresholds = roc_curve(y[val_idx, 4], oof_probas[val_idx, 4])    \n    plot_roc_curve(fpr, tpr, 'NN')\n\n# %% [markdown]\n# ## Precision/Recall Curve\n\n# %% [code]\nif cv:\n    precisions, recalls, thresholds = precision_recall_curve(y[val_idx, 4], oof_probas[val_idx, 4])\n    plot_precision_recall_curve(precisions, recalls, thresholds)\n\n# %% [code]\n#長いので0.506で統一\n\nopt_thresh = 0.506\nprint(opt_thresh)\n\nf = np.median\nmodels = models[-3:]\n\nimport janestreet\nenv = janestreet.make_env()\nfor (test_df, pred_df) in tqdm(env.iter_test()):\n    if test_df['weight'].item() > 0:\n        x_tt = test_df.loc[:, features].values\n        if np.isnan(x_tt[:, 1:].sum()):\n            x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n        \n        ##ニュートライズする場合\n        x_tt = np.append(x_tt, [[1]], axis=1)  # add bias term\n        x_tt = neutralize_array(x_tt, neutralizer)\n        \n        pred = np.mean([model(x_tt, training = False).numpy() for model in models],axis=0)\n        pred = f(pred)\n        pred_df.action = np.where(pred >= opt_thresh, 1, 0).astype(int)\n    else:\n        pred_df.action = 0\n    env.predict(pred_df)\n\n","metadata":{"_uuid":"4de18688-c22f-45e3-a000-d2cfe5f5cc80","_cell_guid":"e259a621-e684-458c-8045-3e2801e971b7","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}