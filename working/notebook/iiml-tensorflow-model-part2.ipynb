{"cells":[{"metadata":{},"cell_type":"markdown","source":"##### ver7　Normalization、onehotencordeinなどにNNを追加\n\n\n\n##### ver8 上記にEarlyStopping追加\n\n\n\n# ver5 epoch 200で提出\n\n下記を参照\nhttps://www.kaggle.com/takeshikobayashi/try-to-use-nn-baseline-78a3ae/edit/run/53017979"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# データロード"},{"metadata":{"trusted":true},"cell_type":"code","source":"# coding: utf-8\n\nimport numpy as np\nimport pandas as pd\n\nfrom tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Activation\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nfrom tensorflow.keras.optimizers import Adam\nimport tensorflow as tf\nimport datatable as dt\n\n#  初始化一个随机数种子\nSEED = 1111\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\n\nprint('Loading data...')\n\ntrain_datatable = dt.fread('/kaggle/input/jane-street-market-prediction/train.csv')  # 用datatable会快一点\ntrain = train_datatable.to_pandas()\ndel train_datatable\n\ndisplay(train)\n\n\n#前処理\nprint('Data preprocessing...')\n\ntrain = train.query('date > 85').reset_index(drop = True)   # 只保留第86天及以后的data\ntrain = train[train['weight'] != 0]\n\ntrain.fillna(train.mean(),inplace=True)  # 注意，这种暴力mean的方案会数据穿越导致overfitting，所以应该滚动mean会更加“正确”，至于效果是不是一定好，可能是个玄学\n\ntrain_date = train['date'].values\n\n# features = []\n# for item in train.columns:\n#     if 'feature' in item:\n#         features.append(item)\n\n# f_mean = np.mean(train[features[1:]].values,axis=0)  # 算一下每个feature的均值，用于缺失值填充\n\nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train['feature_test'] = train['feature_41'] + train['feature_42'] + train['feature_43']\n#train['feature_test']\n\n\n#import matplotlib.pyplot as plt\n\n#fig, ax = plt.subplots(figsize=(15, 5))\n#balance= pd.Series(train['feature_test']).cumsum()\n#ax.set_xlabel (\"Trade\", fontsize=18)\n#ax.set_ylabel (\"Cumulative resp\", fontsize=18);\n#balance.plot(lw=3);\n\n#train['feature_test'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 正規化\nprint('Normalization...')\n\nfeatures_mean = train.loc[:, train.columns.str.contains('feature')].mean()\nfeatures_std  = train.loc[:, train.columns.str.contains('feature')].std()\ntrain_features_normorlization = (train.loc[:, train.columns.str.contains('feature')] - features_mean) / features_std\n\nprint('【train_features_normorlization】【features_mean】【features_std】Done!')\n\ntrain_features_normorlization.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 特徴量追加\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nprint('one-hot...')\n\n#feature_0をone_hot_encorder\n\nf0_onehot_encoder = OneHotEncoder()\nprint('fitting...', end='')\nf0_onehot_encoder.fit(train[['feature_0']])\n\nprint('Done!\\ntransforming...', end='')\nf0_onehot = f0_onehot_encoder.transform(train[['feature_0']]).toarray()\n##DataFrameの場合\n#f0_onehot = f0_onehot_encoder.transform(train[['feature_0']])\nprint('Done!')\nprint(f0_onehot)\n\n# ----------------------------- #\n#     f41_f42_f43 = stock_id    #\n#     \n# ----------------------------- #\n# \n# train['stock_id'] = train['feature_41'] + train['feature_42'] + train['feature_43']\n# \n# train['stock_counts_bin'] = pd.DataFrame({\n#     'stock_counts_bin':pd.qcut(train['stock_id'].rank(method='first'), 20, labels=False)  # 等频20桶\n# })\n\n# stock_id_onehot_encoder = OneHotEncoder(handle_unknown='ignore')\n# print('fitting...', end='')\n# stock_id_onehot_encoder.fit(train[['stock_counts_bin']])\n# print('Done!\\ntransforming...')\n# stock_id_onehot = stock_id_onehot_encoder.transform(train[['stock_counts_bin']]).toarray()\n\n# print('【f0_onehot】【stock_id_onehot】Done!')\n\n\n\n# 看一下各个stock_id出现的次数\ntrain['stock_id'] = train['feature_41'] + train['feature_42'] + train['feature_43']\n# 对该股票历史出现的频次做一个分桶\nstock_counts_bin = pd.DataFrame({\n    'stock_id':train['stock_id'].values,\n    'stock_counts_bin':pd.qcut(train['stock_id'].rank(method='first'), 20, labels=False)  # 等频20桶\n})\n\nstock_id_onehot_encoder = OneHotEncoder(handle_unknown='ignore')\nprint('fitting...', end='')\nstock_id_onehot_encoder.fit(stock_counts_bin[['stock_counts_bin']])\nprint('Done!\\ntransforming...')\nstock_id_onehot = stock_id_onehot_encoder.transform(stock_counts_bin[['stock_counts_bin']]).toarray()\n#stock_id_onehot = stock_id_onehot_encoder.transform(stock_counts_bin[['stock_counts_bin']])\n\nprint(stock_id_onehot[-1])\nprint(stock_id_onehot.shape)\ndisplay(stock_counts_bin)\n\nprint('【f0_onehot】【stock_id_onehot】Done!')\n\n#stock_id_onehot = pd.DataFrame(stock_id_onehot)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stock_id_onehot.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# マージ"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Merge and drop features...')\n\n#train_feature = train.loc[:, train.columns.str.contains('feature')].drop(['feature_0','feature_41','feature_42','feature_43'], axis=1)\ntrain_nparray = np.delete(train_features_normorlization.values, [0,41,42,43], axis=1)\n\nprint(train_nparray.shape)\nprint(f0_onehot.shape)\nprint(stock_id_onehot.shape)\n#print(np.concatenate([train_nparray, f0_onehot, stock_id_onehot],axis=1).shape)\n#print(train_nparray.join(stock_id_onehot).shape)\n\nprint('【train_nparray】Done!')\n\n#f0_onehot = pd.DataFrame(f0_onehot.toarray())\n\n#name = [\"f0_onehot_0\",\"f0_onehot_1\"]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_nparray.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 特徴量と目的変数設定"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Make X and y...')\n\n\nX_train = np.concatenate([train_nparray, f0_onehot, stock_id_onehot],axis=1)\n\n#X_train = np.concatenate([f0_onehot, stock_id_onehot],axis=1)\n\n#X_train = train_feature.join(f0_onehot)\n#X_train = X_train.join(stock_id_onehot)\n\nresp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\ny_train = np.stack([(train[c] > 0).astype('int') for c in resp_cols]).T\n#y_train = pd.DataFrame(y_train)\n\n# print(X_train[0])\ndel train_nparray, f0_onehot, stock_id_onehot, train_features_normorlization\n\nprint('【X_train】【y_train】Done!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# モデル設定（過去のものを転用）"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n### https://www.kaggle.com/takeshikobayashi/iiml-neural-network-starter \n\n\ndef create_mlp(num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate):\n    \n    inp = tf.keras.layers.Input(shape = (num_columns, ))\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n    for i in range(len(hidden_units)): \n        x = tf.keras.layers.Dense(hidden_units[i])(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n        x = tf.keras.layers.Dropout(dropout_rates[i+1])(x)    \n        \n    x = tf.keras.layers.Dense(num_labels)(x)\n    out = tf.keras.layers.Activation('sigmoid')(x)\n    \n    model = tf.keras.models.Model(inputs = inp, outputs = out)\n    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate),\n                  loss = tf.keras.losses.BinaryCrossentropy(label_smoothing = label_smoothing), \n                  metrics = tf.keras.metrics.AUC(name = 'AUC'), \n                 )\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"https://www.kaggle.com/takeshikobayashi/try-to-use-nn-baseline-78a3ae 参照\n\n# 別モデル"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nprint('Create a easy NN model...')\n\nHIDDEN_LAYER = [150, 150, 150]\nTARGET_NUM = 5   # 优化那5个resp\n\n# np.set_printoptions(threshold=1000)\n# print(X_train[0])\n\ninput = tf.keras.layers.Input(shape=(X_train.shape[1], ))\n\nx = tf.keras.layers.BatchNormalization()(input)\nx = tf.keras.layers.Dropout(0.2)(x)\nfor units in HIDDEN_LAYER:\n    x = tf.keras.layers.Dense(units)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)  # 除了ReLU还可以试试别的，后面可以做一个多模型融合\n    x = tf.keras.layers.Dropout(0.2)(x)\nx = tf.keras.layers.Dense(TARGET_NUM)(x)\n\noutput = tf.keras.layers.Activation(\"sigmoid\")(x)\n\nmodel = tf.keras.models.Model(inputs=input, outputs=output)\nmodel.compile(\n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3),\n    metrics   = tf.keras.metrics.AUC(name=\"AUC\"),\n    loss      = tf.keras.losses.BinaryCrossentropy(label_smoothing=1e-2),\n)\n\nprint('Done!')\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nprint('Train NN...')\n\nhistory = model.fit(X_train, y_train, epochs=200, batch_size=5000)\nmodels = []\nmodels.append(model)\n\nprint('Done!')\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nfrom sklearn.model_selection import GroupKFold\nfrom tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nimport gc\n##ver1\nbatch_size = 4096\nhidden_units = [384, 896, 896, 394]\ndropout_rates = [0.10143786981358652, 0.19720339053599725, 0.2703017847244654, 0.23148340929571917, 0.2357768967777311]\nlabel_smoothing = 1e-2\nlearning_rate = 1e-3\n\nmodel = create_mlp(X_train.shape[1], y_train.shape[1], hidden_units, dropout_rates, label_smoothing, learning_rate)\n\nrlr = ReduceLROnPlateau(monitor = 'val_AUC', factor = 0.1, patience = 3, verbose = 0, \n                            min_delta = 1e-4, mode = 'max')\n#ckp = ModelCheckpoint(ckp_path, monitor = 'val_AUC', verbose = 0, \n#                          save_best_only = True, save_weights_only = True, mode = 'max')\nes = EarlyStopping(monitor = 'val_AUC', min_delta = 1e-4, patience = 7, mode = 'max', \n                       baseline = None, restore_best_weights = True, verbose = 0)\n\nmodel.fit(X_train, y_train, epochs = 1000)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GroupKFold\nfrom tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nimport gc\n\n##ver1\nbatch_size = 4096\nhidden_units = [384, 896, 896, 394]\ndropout_rates = [0.10143786981358652, 0.19720339053599725, 0.2703017847244654, 0.23148340929571917, 0.2357768967777311]\nlabel_smoothing = 1e-2\nlearning_rate = 1e-3\n\n#oof = np.zeros(len(train['action']))\noof = np.zeros(len(y_train))\n\ngkf = GroupKFold(n_splits = 5)\n\n#for fold, (tr, te) in enumerate(gkf.split(X_train, y_train, train['date'].values)):\nfor fold, (tr, te) in enumerate(gkf.split(X_train, y_train, train_date)):\n    \n    #X_tr, X_val = train.loc[tr, features].values, train.loc[te, features].values\n    #y_tr, y_val = train.loc[tr, 'action'].values, train.loc[te, 'action'].values\n    \n    #X_tr = X_train.iloc[tr, : ].values\n    #X_val = X_train.iloc[te, : ].values\n    \n    #y_tr = y_train.iloc[tr, :].values\n    #y_val = y_train.iloc[te, :].values\n    \n    X_tr = X_train[tr, : ]\n    X_val = X_train[te, : ]\n    \n    y_tr = y_train[tr, :]\n    y_val = y_train[te, :]\n    \n    \n    ckp_path = f'JSModel_{fold}.hdf5'\n    \n    model = create_mlp(X_tr.shape[1], y_tr.shape[1], hidden_units, dropout_rates, label_smoothing, learning_rate)\n    \n    rlr = ReduceLROnPlateau(monitor = 'val_AUC', factor = 0.1, patience = 3, verbose = 0, \n                            min_delta = 1e-4, mode = 'max')\n    ckp = ModelCheckpoint(ckp_path, monitor = 'val_AUC', verbose = 0, \n                          save_best_only = True, save_weights_only = True, mode = 'max')\n    es = EarlyStopping(monitor = 'val_AUC', min_delta = 1e-4, patience = 7, mode = 'max', \n                       baseline = None, restore_best_weights = True, verbose = 0)\n    \n    model.fit(X_tr, y_tr, validation_data = (X_val, y_val), epochs = 1000, \n              batch_size = batch_size, callbacks = [rlr, ckp, es], verbose = 0)\n    #model.fit(X_tr, y_tr)\n              \n    oof[te] += model.predict(X_val, batch_size = batch_size * 4).ravel()\n    score = roc_auc_score(y_val, oof[te])\n    print(f'Fold {fold} ROC AUC:\\t', score)\n    \n    # Finetune 3 epochs on validation set with small learning rate\n    model = create_mlp(X_tr.shape[1], y_tr.shape[1], hidden_units, dropout_rates, label_smoothing, learning_rate)\n    model.load_weights(ckp_path)\n    model.fit(X_val, y_val, epochs = 3, batch_size = batch_size, verbose = 0)\n    model.save_weights(ckp_path)\n   \n    K.clear_session()\n    del model\n    rubbish = gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_tr.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_val.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_tr.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 実験"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n\n\nfrom sklearn.model_selection import train_test_split\n \nX_train2, X_test2, y_train2, y_test2 = train_test_split(X_train,\n                                                        y_train, \n                                                        test_size = 0.2,\n                                                        train_size = 0.8,#教師データ少なくなるのが怖いので4:1で分割\n                                                        stratify = y_train)\n\n\nbatch_size = 4096\nhidden_units = [384, 896, 896, 394]\ndropout_rates = [0.10143786981358652, 0.19720339053599725, 0.2703017847244654, 0.23148340929571917, 0.2357768967777311]\nlabel_smoothing = 1e-2\nlearning_rate = 1e-3\n\n\n#モデルの構造\nmodel2 = Sequential()\nmodel2.add(tf.keras.layers.Input(shape = (X_train2.shape[1], )))\nmodel2.add(tf.keras.layers.BatchNormalization())\nmodel2.add(tf.keras.layers.Dropout(0.2))\nmodel2.add(tf.keras.layers.Dense(150))\nmodel2.add(tf.keras.layers.BatchNormalization())\nmodel2.add(tf.keras.layers.Activation(tf.keras.activations.swish))\nmodel2.add(tf.keras.layers.Dropout(0.2))\nmodel2.add(tf.keras.layers.Dense(150))\nmodel2.add(tf.keras.layers.BatchNormalization())\nmodel2.add(tf.keras.layers.Activation(tf.keras.activations.swish))\nmodel2.add(tf.keras.layers.Dropout(0.2))\nmodel2.add(tf.keras.layers.Dense(5))\nmodel2.add(tf.keras.layers.Activation('sigmoid'))\n\n\nmodel2.compile(loss = tf.keras.losses.BinaryCrossentropy(label_smoothing = label_smoothing),\n              optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate),\n              metrics = tf.keras.metrics.AUC(name = 'AUC'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nhist = model2.fit(X_train2, y_train2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train2.shape)\nprint(X_test2.shape)\nprint(y_train2.shape)\nprint(y_test2.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n\nbatch_size = 4096\nhidden_units = [384, 896, 896, 394]\ndropout_rates = [0.10143786981358652, 0.19720339053599725, 0.2703017847244654, 0.23148340929571917, 0.2357768967777311]\nlabel_smoothing = 1e-2\nlearning_rate = 1e-3\n\n\n\n#rlr = ReduceLROnPlateau(monitor = 'val_AUC', factor = 0.1, patience = 3, verbose = 0, \n#                       min_delta = 1e-4, mode = 'max')\n#ckp = ModelCheckpoint(ckp_path, monitor = 'val_AUC', verbose = 0, \n#                      save_best_only = True, save_weights_only = True, mode = 'max')\n#es = EarlyStopping(monitor = 'val_AUC', min_delta = 1e-4, patience = 7, mode = 'max', \n#                   baseline = None, restore_best_weights = True, verbose = 0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = y_test2[4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traintest = y_train2[4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model = create_mlp(X_train2.shape[1], 1, hidden_units, dropout_rates, label_smoothing, learning_rate)\nmodel = create_mlp(X_train2.shape[1], y_train2.shape[1], hidden_units, dropout_rates, label_smoothing, learning_rate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train2, y_train2, validation_data = (X_test2, y_test2), epochs = 200, batch_size = batch_size, verbose = 0)\n\n#model.fit(X_test2, test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_tr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#x = tf.keras.layers.Dense(TARGET_NUM)(x)\n#\n#output = tf.keras.layers.Activation(\"sigmoid\")(x)\n#\n#model = tf.keras.models.Model(inputs=input, outputs=output)\n#model.compile(\n#    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3),\n#    metrics   = tf.keras.metrics.AUC(name=\"AUC\"),\n#    loss      = tf.keras.losses.BinaryCrossentropy(label_smoothing=1e-2),\n#)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef create_mlp(num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate):\n    \n    inp = tf.keras.layers.Input(shape = (num_columns, ))\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n    \n    for i in range(len(hidden_units)): \n        x = tf.keras.layers.Dense(hidden_units[i])(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n        x = tf.keras.layers.Dropout(dropout_rates[i+1])(x)    \n        \n    x = tf.keras.layers.Dense(num_labels)(x)\n    out = tf.keras.layers.Activation(\"sigmoid\")(x)\n    #out = tf.keras.layers.Activation(\"softmax\")(x)\n    \n    \n    \n    \n    model = tf.keras.models.Model(inputs = inp, outputs = out)\n    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate),\n                  loss = tf.keras.losses.BinaryCrossentropy(label_smoothing = label_smoothing), \n                  metrics = tf.keras.metrics.AUC(name = 'AUC'), \n                 )\n    \n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\ndef create_mlp2(num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate):\n\n    input = tf.keras.layers.Input(shape=(num_columns, ))\n    \n    x = tf.keras.layers.BatchNormalization()(input)\n    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n    \n    for units in range(len(hidden_units)):\n        x = tf.keras.layers.Dense(hidden_units[i])(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n        x = tf.keras.layers.Dropout(dropout_rates[i+1])(x)\n        \n    x = tf.keras.layers.Dense(num_labels)(x)\n    \n    output = tf.keras.layers.Activation(\"sigmoid\")(x)\n    \n    return model\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##ver1\nbatch_size = 4096\nhidden_units = [384, 896, 896, 394]\ndropout_rates = [0.10143786981358652, 0.19720339053599725, 0.2703017847244654, 0.23148340929571917, 0.2357768967777311]\nlabel_smoothing = 1e-2\nlearning_rate = 1e-3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train = True\n\n#ver8 EarlyStopping追加\n\nif Train:\n    \n    from keras.callbacks import EarlyStopping\n    \n#early_stopping =  EarlyStopping(\n#                            monitor='loss',\n#                            min_delta=0.000,\n#                            patience=10,\n#)\n    \n    early_stopping = EarlyStopping(monitor = 'val_AUC', min_delta = 1e-4, patience = 7, mode = 'max', \n                           baseline = None, restore_best_weights = True, verbose = 0)\n    \n    print('Train NN...')\n    \n    model = create_mlp(X_train.shape[1], y_train.shape[1], hidden_units, dropout_rates, label_smoothing, learning_rate)\n    \n    \n    history = model.fit(X_train, y_train, epochs=200, batch_size=batch_size,callbacks=[early_stopping]) # CallBacksに設定\n    models = []\n    models.append(model)\n    \n    print('Done!')\n    \nelse:\n    \n    model = create_mlp(X_train.shape[1], y_train.shape[1], hidden_units, dropout_rates, label_smoothing, learning_rate)\n    model.load_weights('../input/iimlnnmodel-part2/NNmodel_part2.hdf5')\n    \n    \n    models = []\n    models.append(model)\n    \n    print('weight load Done!')\n    \n#    for i in range(num_models):\n#        clf = create_mlp(len(features), 1, hidden_units, dropout_rates, label_smoothing, learning_rate)\n#        clf.load_weights(f'./JSModel_{i}.hdf5')\n#     clf.load_weights(f'./JSModel_{i}.hdf5')\n#        models.append(clf)   \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#重みだけ保存\nmodel.save_weights('NNmodel_part2.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nfrom sklearn.model_selection import GroupKFold\n\noof = np.zeros(len(y_train))\ngkf = GroupKFold(n_splits = 5)\nfor fold, (tr, te) in enumerate(gkf.split(y_train, y_train, X_train['date'].values)):\n    \n    X_tr, X_val = X_train.loc[tr, features].values, X_train.loc[te, features].values\n    y_tr, y_val = X_train.loc[tr, 'action'].values, X_train.loc[te, 'action'].values\n  \n    ckp_path = f'JSModel_{fold}.hdf5'\n    \n    model = create_mlp(X_tr.shape[1], 1, hidden_units, dropout_rates, label_smoothing, learning_rate)\n    \n    rlr = ReduceLROnPlateau(monitor = 'val_AUC', factor = 0.1, patience = 3, verbose = 0, \n                            min_delta = 1e-4, mode = 'max')\n    ckp = ModelCheckpoint(ckp_path, monitor = 'val_AUC', verbose = 0, \n                          save_best_only = True, save_weights_only = True, mode = 'max')\n    es = EarlyStopping(monitor = 'val_AUC', min_delta = 1e-4, patience = 7, mode = 'max', \n                       baseline = None, restore_best_weights = True, verbose = 0)\n    model.fit(X_tr, y_tr, validation_data = (X_val, y_val), epochs = 1000, \n              batch_size = batch_size, callbacks = [rlr, ckp, es], verbose = 0)\n              \n    oof[te] += model.predict(X_val, batch_size = batch_size * 4).ravel()\n    score = roc_auc_score(y_val, oof[te])\n    print(f'Fold {fold} ROC AUC:\\t', score)\n  \n    # Finetune 3 epochs on validation set with small learning rate\n    model = create_mlp(X_tr.shape[1], 1, hidden_units, dropout_rates, label_smoothing, learning_rate / 100)\n    model.load_weights(ckp_path)\n    model.fit(X_val, y_val, epochs = 3, batch_size = batch_size, verbose = 0)\n    model.save_weights(ckp_path)\n   \n    K.clear_session()\n    del model\n    rubbish = gc.collect()\n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 久野ツール関数定義"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nenv_called = False\niter_test = None\n\ndef calc_score(train_df, pred_df):\n    train_test_result = train_df[[\"date\", \"weight\", \"resp\"]].copy().reset_index(drop=True)\n    train_test_result[\"resp_weight_action\"] = pred_df[\"action\"] * train_test_result[\"weight\"] * train_test_result[\"resp\"] \n    px = train_test_result.groupby(\"date\")[\"resp_weight_action\"].sum()\n    num_of_i = len(px)\n    t = sum(px)/np.sqrt(sum(px ** 2)) * np.sqrt(250/num_of_i)\n    u = min(max(t, 0), 6) * sum(px)\n    return u\n\ndef pred_action(test_df, models, f=np.median, th=0.5):\n        ##特徴量追加分　追記\n            \n        add_feature(test_df)\n            \n        #test_df['feature_cross_41_42_43'] = test_df['feature_42'] +  test_df['feature_43'] +  test_df['feature_44']\n        #test_df['feature_cross_1_2'] = test_df['feature_1'] / (test_df['feature_2'] + 1e-5)\n        \n        ##以上\n        if test_df['weight'].item() <=0:\n            return 0\n        else:\n            \n            \n            \n            x_tt = test_df.loc[:, features].values\n            if np.isnan(x_tt[:, 1:].sum()):\n                #print(f_mean)\n                x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n            pred = np.mean([model(x_tt, training = False).numpy() for model in models],axis=0)\n            pred = f(pred)\n            return np.where(pred >= th, 1, 0).astype(int)\n\ndef pred_action_burst(test_df, models, f=np.median, th=0.5):\n        x_tt = test_df.loc[:, features].values\n        if np.isnan(x_tt[:, 1:].sum()):\n            #print(f_mean)\n            x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n        pred = np.mean([model(x_tt, training = False).numpy() for model in models],axis=0)\n        pred = f(pred)\n        pred = np.where(pred >= th, 1, 0).astype(int)\n        fil = (test_df['weight'] == 0)\n        pred[fil] = 0\n        return pred\n\ndef submit(models, example_test_df=None, burst=False):\n    if example_test_df is None:\n        print(\"submit mode\")\n        global iter_test\n        global env_called\n        if not env_called:\n            import janestreet\n            env = janestreet.make_env()\n            iter_test = env.iter_test()\n            env_called = True\n    else:\n        print(\"example test mode\")\n        # 事前準備\n        print(\"preparing dataset...\")\n        if not burst:\n            test_iter = [pd.DataFrame([s[1]]) for s in example_test_df.iterrows()] \n            pred_iter = [pd.DataFrame([0], columns=[\"action\"]) for _ in range(len(example_test_df))]\n            iter_test = zip(test_iter, pred_iter)\n        else:\n            print(\"bust mode (it's fast but it doesn't use time-serires module. Check pred_action_burst carefully.)\")\n            batch_size = 4096\n            row_len = len(example_test_df)\n            test_iter = [example_test_df.iloc[i * batch_size: (i+1) * batch_size ] for i in range(row_len//batch_size + 1)] \n            pred_iter = [pd.DataFrame([0] * batch_size, columns=[\"action\"]) for i in range(row_len//batch_size + 1)]\n            pred_iter[-1] = pd.DataFrame([0] * (row_len%batch_size), columns=[\"action\"])\n\n            iter_test = zip(test_iter, pred_iter)\n        print(\"preparing dataset... Done\")\n\n    f = lambda x: np.median(x, axis=1)\n    th = 0.5\n    models = models[-3:]\n    \n    print(\"predicting...\")\n    if example_test_df is None:\n        for (test_df, pred_df) in tqdm(iter_test):\n            \n            ##特徴量追加分　追記\n            \n            add_feature(test_df)\n            \n            ##以上\n            \n            pred_df.action = pred_action(test_df, models, f, th)\n            env.predict(pred_df)\n        return pd.DataFrame()\n    elif not burst:\n        submit_ex_result = []\n        for (test_df, pred_df) in tqdm(iter_test):\n            \n            ##特徴量追加分　追記\n            \n            add_feature(test_df)\n            \n            ##以上\n            \n            pred_df.action = pred_action(test_df, models, f, th)\n            submit_ex_result.append(pred_df)  # env.predict(pred_df)\n        return pd.concat(submit_ex_result).reset_index(drop=True)\n    else:\n        submit_ex_result = []\n        for (test_df, pred_df) in tqdm(iter_test):\n            \n            ##特徴量追加分　追記\n            add_feature(test_df)\n            \n            ##以上\n            \n            pred_df.action = pred_action_burst(test_df, models, f, th)\n            submit_ex_result.append(pred_df)  # env.predict(pred_df)\n        return pd.concat(submit_ex_result).reset_index(drop=True)  \n    \n    \ndef add_feature(test_df):\n    \n        #test_sample = test_df.loc[:, test_df.columns.str.contains('feature')]\n        test_sample = test_df\n        \n        #feature_list = test_sample.columns\n        #features\n        \n        #test_sample = test_sample.values\n        \n        # 如果这行样本里面有NaN的话（忽略离散特征feature_0）用train数据集里的mean替换NaN\n        if np.isnan(test_sample.sum()):         \n            test_sample = np.nan_to_num(test_sample) + np.isnan(test_sample) * features_mean.values\n        \n        # Normalization\n        test_sample = (test_sample - features_mean.values) / features_std.values\n        \n        #NumpyからPandasに戻す\n        test_sample = pd.DataFrame(test_sample,columns = features)\n        \n        \n        # one-hot\n        test_f0_onehot = f0_onehot_encoder.transform(test_df[['feature_0']]).toarray()\n        \n        #stockid_df = test_sample['feature_41'] + test_sample['feature_42'] + test_sample['feature_43']\n        \n        test_sample['stock_id'] = test_sample['feature_41'] + test_sample['feature_42'] + test_sample['feature_43']\n        #test_sample_stock_id = test_sample[0,41] + test_sample[0,42] + test_sample[0,43]\n        \n        \n        #stock_counts_bin = pd.DataFrame({\n        #    'stock_id':test_sample['stock_id'].values,\n        #    'stock_counts_bin':pd.qcut(test_sample['stock_id'].rank(method='first'), 20, labels=False)  # 等频20桶\n        #    })\n        \n        \n        test_stock_id_onehot = stock_id_onehot_encoder.transform(test_sample[['stock_id']]).toarray()\n        #test_stock_id_onehot = stock_id_onehot_encoder.transform(test_sample[['stock_counts_bin']]).toarray()\n        stock_counts_bin\n        \n        test_sample = test_sample.drop(['stock_id'],axis = 1)\n        \n        # drop and merge\n        test_sample = np.delete(test_sample.values, [0,41,42,43], axis=1)\n        test_sample = np.concatenate([test_sample, test_f0_onehot, test_stock_id_onehot],axis=1)\n        \n        return test_df\n        \n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n%%time\n\n#feature入れなおし\n#features = [c for c in X_train.columns if 'feature' in c]\n\n#fmean 入れなおし\n#f_mean = X_train[features[1:]].mean().values\n\nf_mean = features_mean\n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# predict"},{"metadata":{"trusted":true},"cell_type":"code","source":"## 変更点\n## numpyばかりでよくわからんからpandasに変更する。\n\nTHRESHOLD = 0.5   # 试了一个对照组，还是0.5比较好用\n\nimport janestreet\nfrom tqdm import tqdm\njanestreet.make_env.__called__ = False\nenv = janestreet.make_env()\n\nprint(features_mean.values)\n\ni = 1\n\nfor (test_df, pred_df) in tqdm(env.iter_test()):\n    \n    \n    if test_df['weight'].item() > 0:\n        \n        #test_sample = test_df.loc[:, test_df.columns.str.contains('feature')].values  \n        test_sample = test_df.loc[:, test_df.columns.str.contains('feature')]\n        \n        feature_list = test_sample.columns\n        \n        test_sample = test_sample.values\n        \n        # 如果这行样本里面有NaN的话（忽略离散特征feature_0）用train数据集里的mean替换NaN\n        if np.isnan(test_sample.sum()):         \n            test_sample = np.nan_to_num(test_sample) + np.isnan(test_sample) * features_mean.values\n        \n        # Normalization\n        test_sample = (test_sample - features_mean.values) / features_std.values\n        \n        #NumpyからPandasに戻す\n        test_sample = pd.DataFrame(test_sample,columns = feature_list)\n        \n        \n        # one-hot\n        test_f0_onehot = f0_onehot_encoder.transform(test_df[['feature_0']]).toarray()\n        \n        #stockid_df = test_sample['feature_41'] + test_sample['feature_42'] + test_sample['feature_43']\n        \n        test_sample['stock_id'] = test_sample['feature_41'] + test_sample['feature_42'] + test_sample['feature_43']\n        #test_sample_stock_id = test_sample[0,41] + test_sample[0,42] + test_sample[0,43]\n        \n        \n        #stock_counts_bin = pd.DataFrame({\n        #    'stock_id':test_sample['stock_id'].values,\n        #    'stock_counts_bin':pd.qcut(test_sample['stock_id'].rank(method='first'), 20, labels=False)  # 等频20桶\n        #    })\n        \n        \n        test_stock_id_onehot = stock_id_onehot_encoder.transform(test_sample[['stock_id']]).toarray()\n        #test_stock_id_onehot = stock_id_onehot_encoder.transform(test_sample[['stock_counts_bin']]).toarray()\n        stock_counts_bin\n        \n        test_sample = test_sample.drop(['stock_id'],axis = 1)\n        \n        # drop and merge\n        test_sample = np.delete(test_sample.values, [0,41,42,43], axis=1)\n        test_sample = np.concatenate([test_sample, test_f0_onehot, test_stock_id_onehot],axis=1)\n        \n        #確認用\n        if i == 1:\n        \n            check = test_sample\n        \n        else:\n            check = np.append(check,test_sample, axis=0)\n        \n        #予測\n        pred = model(test_sample, training = False).numpy()\n        \n        # version = 1\n        pred = np.median(pred)\n        \n        i = i + 1\n        \n        # version = 2\n#         pred = np.mean(pred)\n        \n        # version = 3\n#         pred = pred[3]\n        \n        pred_df.action = np.where(pred >= THRESHOLD, 1, 0).astype(int)\n    else:\n        pred_df.action = 0\n    env.predict(pred_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"check","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred2 = model(check, training = False).numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}